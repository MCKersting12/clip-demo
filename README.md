# clip-demo
Quick demo of the CLIP model


The CLIP model was proposed in [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)
This project demonstrates CLIP as it is applied to a zero-shot image classification task

The only dependency you should need is docker

You can start the demo by running docker-compose up in the root of the project and navigating to localhost:8501
